{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKMAiZgRuFvP"
      },
      "source": [
        "\n",
        "# INFO 159/259\n",
        "#<center> Homework 2: Featurized Models for Sentiment Analysis </center>\n",
        "\n",
        "<center> Due: February 10, 2026 @ 11:59pm </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWnk8-2suFvP"
      },
      "source": [
        "For this assignment, we provide an implementation of a simple binary classifier that will predict the sentiment of sentences from Yelp reviews based on a group of original features -- provided by you!\n",
        "\n",
        "Before diving into any code, please read through the associated instructions for an overview of the assignment and specific instructions on how to submit."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment Overview and Learning Objectives"
      ],
      "metadata": {
        "id": "13Uk9VJx9KZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, you will\n",
        "1. write code that generates features for the positive/negative sentiment classification of sentences from Yelp reviews\n",
        "2. train a model on those features to predict the sentiment of new data."
      ],
      "metadata": {
        "id": "uzWL6fiI9QCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, a basic logistic regression model (from the `scikit-learn` package) is trained with your\n",
        "created features to predict each review's sentiment.\n",
        "\n",
        "> Your ultimate goal for this assignment is to **featurize the text creatively** and **optimize the accuracy** of the model on the test data.\n",
        "\n",
        "Your main tasks are to implement:\n",
        "\n",
        "1. A bag of words feature. Fill out the `bag_of_words` function in the notebook to represent a document through its bag of words. Represent each **lowercase** word tokenized with the NLTK `word_tokenize` function through its binary value. Remember that the feature value for a word that shows up in a review should be 1, no matter how many times it is mentioned. Please keep your code between the `#BEGIN`/`#END SOLUTION` flags.\n",
        "\n",
        "2. Create **three** different classes of features. Implement them in the `feature1`, `feature2`, and\n",
        "`feature3` functions. Create features that you think would perform better than bag of words, and assess their independent performance on the development data. Describe your reasoning and include accuracy scores on the `dev.txt` data (printed in the Colab output). **You must report your reasoning and development scores in the table provided.**\n",
        "\n",
        "It is not required that your features actually perform well, but your justification and reasoning should be defensible. We are looking for thoughtful insights as to why your original features should out-perform the simple bag of words implementation."
      ],
      "metadata": {
        "id": "6pltbwPG9gIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more feature ideas consult [SLP Ch. 4](https://web.stanford.edu/~jurafsky/slp3/4.pdf) and lecture slides, and please do keep the following constraints\n",
        "in mind:\n",
        "- You should only change the code for `bag_of_words`, `feature1`, `feature2`, `feature3` and `combiner_function` along with any code you need to support those functions. You must use the classifier provided.\n",
        "- You are free to use external dictionaries (such as LIWC or AFINN); if you use these, upload them to [Gradescope](https://www.gradescope.com/courses/1238346) as well with your `HW2.ipynb`.\n",
        "- Do not use pre-trained word vectors (including static embeddings like word2vec or contextual\n",
        "embeddings like BERT).\n",
        "- Do not use the predictions of any other supervised model or LLM on this data as a feature for logistic regression (e.g., do not train a separate CNN on this data, make predictions on the dev data using the CNN, and then treat your CNN prediction as a feature).\n",
        "- Do not alter `{train,dev,test}.txt` or find additional sources of training data. This homework\n",
        "is focusing on how you represent your data, not other aspects of the training regime.\n",
        "- Do not import any other additional libraries."
      ],
      "metadata": {
        "id": "NPtKgJrn_X6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of the notebook we've  included several other methods that you can use to interrogate your model; please feel free to use these to assess what kind of mistakes your current model is making in order to help brainstorm new features.\n",
        "\n",
        "Once you've defined your features, described them, and evaluated their performance on the development data, it's time to make predictions on the test data. Adapt the `combiner_function` cell to include whichever features you like and execute it to make predictions on the test data. This will create the file `combiner_function_predictions.csv`; download this from Colab (using e.g. the file manager on the left panel) and submit it to [Gradescope](https://www.gradescope.com/courses/1238346), along with your `HW2.ipynb` and a pdf of the notebook."
      ],
      "metadata": {
        "id": "UjCab-EB_xwN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "outputs": [],
      "source": [
        "import sys, argparse\n",
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import nltk\n",
        "import csv\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas import option_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2NOIY02YxlOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8a211b-5629-4a21-da97-ec0b6b31844f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt_tab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCsC_2y3uFvQ"
      },
      "source": [
        "### Intro: Gather Data + Create Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdSUxWgvuFvQ"
      },
      "source": [
        "#### Gather Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hk07KCgwoZy"
      },
      "source": [
        "Let's download the data we'll use for training and development, and also the data we'll make predictions with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hn0XtfFeqP2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06271949-716a-46cd-fe80-1f551d55a402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-10 15:57:08--  https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/train.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/data/train.txt [following]\n",
            "--2026-02-10 15:57:09--  https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/data/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3621775 (3.5M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]   3.45M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2026-02-10 15:57:09 (266 MB/s) - ‘train.txt’ saved [3621775/3621775]\n",
            "\n",
            "--2026-02-10 15:57:09--  https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/test.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/data/test.txt [following]\n",
            "--2026-02-10 15:57:10--  https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/data/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 207117 (202K) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>] 202.26K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2026-02-10 15:57:10 (57.6 MB/s) - ‘test.txt’ saved [207117/207117]\n",
            "\n",
            "--2026-02-10 15:57:10--  https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/dev.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/data/dev.txt [following]\n",
            "--2026-02-10 15:57:11--  https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/data/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 214630 (210K) [text/plain]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>] 209.60K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2026-02-10 15:57:11 (63.0 MB/s) - ‘dev.txt’ saved [214630/214630]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get data\n",
        "!wget https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/train.txt -O train.txt\n",
        "!wget https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/test.txt -O test.txt\n",
        "!wget https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/dev.txt -O dev.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jq2yq0xpRCUb"
      },
      "outputs": [],
      "source": [
        "trainingFile = \"train.txt\"\n",
        "evaluationFile = \"dev.txt\"\n",
        "testFile = \"test.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OJsUNIUuFvR"
      },
      "source": [
        "#### Define Classifier class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXBIDQFMuFvR"
      },
      "source": [
        "Next, we've created a Binary Classifier. This class will let us learn the traits associated with positive and negatively classed review sentences in order to make predictions on our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the classification class which\n",
        "## loads the data and sets up the model.\n",
        "######################################################################\n",
        "\n",
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, L2_regularization_strength=1.0, min_feature_count=1):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.log_reg = None\n",
        "        self.L2_regularization_strength=L2_regularization_strength\n",
        "        self.min_feature_count=min_feature_count\n",
        "\n",
        "        self.trainX, self.trainY, self.trainOrig = self.process(trainingFile, training=True)\n",
        "        self.devX, self.devY, self.devOrig = self.process(evaluationFile, training=False)\n",
        "        self.testX, _, self.testOrig = self.process(testFile, training=False)\n",
        "\n",
        "    # Read data from file\n",
        "    def load_data(self, filename):\n",
        "        data = []\n",
        "        with open(filename, encoding=\"utf8\") as file:\n",
        "            reader = csv.reader(file, delimiter='\\t')\n",
        "            for row in reader:\n",
        "                idd = row[0]\n",
        "                label = row[1]\n",
        "                text = row[2]\n",
        "\n",
        "                data.append((idd, label, text))\n",
        "\n",
        "        return data\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, dataFile, training = False):\n",
        "        original_data = self.load_data(dataFile)\n",
        "        data = self.featurize(original_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (label, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = label\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        data = self.load_data(dataFile)\n",
        "        data = self.featurize(data)\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (data_id, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = data_id\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    @ignore_warnings(category=ConvergenceWarning)\n",
        "    def evaluate(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        self.log_reg = linear_model.LogisticRegression(C = self.L2_regularization_strength, max_iter=1000)\n",
        "        self.log_reg.fit(self.trainX, self.trainY)\n",
        "        training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "        development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "        print(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\n",
        "\n",
        "    # Predict labels for new data\n",
        "    def predict(self):\n",
        "        predX = self.log_reg.predict(self.testX)\n",
        "\n",
        "        with open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\") as out:\n",
        "            writer=csv.writer(out)\n",
        "            writer.writerow([\"Id\", \"Expected\"])\n",
        "            for idx, data_id in enumerate(self.testX):\n",
        "                writer.writerow([self.testOrig[idx][0], predX[idx]])\n",
        "        out.close()\n",
        "\n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oeXPdTguFvR"
      },
      "source": [
        "#### Simple Classifier example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example of select sentences with labels:\n",
        "- `Positive` (pos): My last mani lasted almost a month with no chips.\n",
        "- `Negative` (neg): We went tonight and are now kicking ourselves."
      ],
      "metadata": {
        "id": "MP6kbJqw_PnG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmfkG782kgo"
      },
      "source": [
        "Let's create an initial classifier based on a really simple feature using a dictionary:\n",
        "\n",
        "* if the abstract contains the words \"love\" or \"like\", the `contains_positive_word` feature will fire, and\n",
        "* if it contains either \"hate\" or \"dislike\", the `contains_negative_word` will fire.  \n",
        "\n",
        "Note how we use `nltk.word_tokenize` to tokenize the text into its discrete words (the documentation for which can be found [here](https://www.nltk.org/api/nltk.tokenize.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xCq1bL3e2jUj"
      },
      "outputs": [],
      "source": [
        "def simple_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        if word == \"love\" or word == \"like\":\n",
        "            feats[\"contains_positive_word\"] = 1\n",
        "        if word == \"hate\" or word == \"dislike\":\n",
        "            feats[\"contains_negative_word\"] = 1\n",
        "\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3PQdN9r3Ujz"
      },
      "source": [
        "Now let's see how that feature performs on the development data.\n",
        "\n",
        "Note the `L2_regularization_strength` parameter specifies the strength of the L2 regularizer (values closer to 0 = stronger regularization), and `min_feature_count` specifies how many data points need to contain a feature for it to be passed into the model as a feature. Both are ways to prevent the model from overfitting and achieve higher performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jnqjxd6fKPiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ded833a-e68f-4291-f7da-9fb66095e26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: simple_featurize, Features: 2, Train accuracy: 0.596, Dev accuracy: 0.501\n"
          ]
        }
      ],
      "source": [
        "simple_classifier = Classifier(simple_featurize, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "simple_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4XQzU3PdeU"
      },
      "source": [
        "So we've created a classifier. But is its accuracy score any good?  Let's calculate the accuracy of a \"majority classifier\" to provide some context. This determines the most-represented (majority) class in the training data, and then predicts every test point to be this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8t--LfOjPj7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c8be24-8827-46ab-b9c4-70d7602106dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Majority class: pos\tDev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "def majority_class(trainY, devY):\n",
        "    labelCounts=Counter()\n",
        "    for label in trainY:\n",
        "        labelCounts[label]+=1\n",
        "    majority_class=labelCounts.most_common(1)[0][0]\n",
        "\n",
        "    correct=0.\n",
        "    for label in devY:\n",
        "        if label == majority_class:\n",
        "            correct+=1\n",
        "\n",
        "    print(\"Majority class: %s\\tDev accuracy: %.3f\" % (majority_class, correct/len(devY)))\n",
        "majority_class(simple_classifier.trainY, simple_classifier.devY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1TTF-lkuFvS"
      },
      "source": [
        "The feature we created in `simple_featurize`, evidently, doesn't have a whole lot of legs. In the next portion of the homework, you'll be designing a few features of your own in the hopes of achieving the highest accuracy possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIEkYOWO5ClC"
      },
      "source": [
        "## Deliverable 1\n",
        "\n",
        "Your job in this homework is to implement a binary bag-of-words model (i.e., one that assigns a feature value of 1 to each word type that is present in the text); and to brainstorm **3 additional** distinct classes of features, justify why they might help improve the performance *over a bag of words* for this task, implement them in code, and then assess their independent performance on the development data.\n",
        "\n",
        "<!-- Do not remove this comment, it is used by the autograder -->\n",
        "<!-- Only modify the text within the placeholders [INSERT DESCRIPTION HERE]) -->\n",
        "\n",
        "\n",
        "Describe your new features, one paragraph (around 50 words) each:   \n",
        "\n",
        "**Bag of words (Example)**: The bag-of-words feature converts review text into binary indicators that signal the presence or absence of each word, without considering the order. This approach simplifies text representation, enabling the classifier to focus on distinguishing words for each review.\n",
        "\n",
        "\n",
        "**Feature 1**: [For my first feature, I used bigrams or pairs of words. I thought that this could pick up negation and be able to distinguish common patterns like not good and very good. It also preserves some word order, and picks up common phrases that may carry sentiment like \"highly recommend\" for example.]\n",
        "\n",
        "**Feature 2**: [For my second feature, I considered negation. As traditional bag of words is not able to consider word order, the negation words are missed. Also, if there are multiple words in between the negator, the bigram feature will not pick it up. So this feature is a binary indicator of the presence of negation words.]\n",
        "\n",
        "**Feature 3**: [The third feature is a word count feature. Keywords that indicate sentiment may occur multiple times. The model will be able to distinguish commonly occuring words, and the performance will not be affected by them. Emphasis and intensity are captured from repitition, and signal is improved when key words repeat.]\n",
        "\n",
        "Now, implement the features in the specified `bag_of_words`, `feature1`, `feature2`, and `feature3` functions, and execute each respective classifier to show its performance.\n",
        "\n",
        "Note that it is not required for your features to actually perform well, but your justification for why it *should* perform better than a bag of words should be defensible.  Consider the type of data you are working with: what do you look for when writing/reading a review?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yza5IRI5uFvS"
      },
      "source": [
        "### Implement Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vVl1zAREekC3"
      },
      "outputs": [],
      "source": [
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "    feats = {}\n",
        "    # BEGIN SOLUTION\n",
        "    words = nltk.word_tokenize(text)\n",
        "    for w in set (words):\n",
        "      w = w.lower()\n",
        "      feats[w] = 1\n",
        "\n",
        "    return feats\n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_3AJ5qMBeqmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1870d8-3c72-40b8-db11-2008dc68fc45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: bag_of_words, Features: 22970, Train accuracy: 0.848, Dev accuracy: 0.731\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "bow_classifier = Classifier(bag_of_words, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "bow_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you correctly implemented `bag_of_words`, you may expect a train accuracy of ~0.849 and a dev accuracy of ~0.732."
      ],
      "metadata": {
        "id": "oZb4TZmiBMwU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oImk4AguFvS"
      },
      "source": [
        "### Implement Original Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ocPMYhIt4BX0"
      },
      "outputs": [],
      "source": [
        "def feature1(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "    # BEGIN SOLUTION\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for w1, w2 in set(zip(words, words[1:])):\n",
        "      w1 = w1.lower()\n",
        "      w2 = w2.lower()\n",
        "      feats[w1, w2] = 1\n",
        "    return feats\n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-MAwRwbQ7lVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae8ebb3b-56e7-4d1e-9f08-2bbac07d4897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature1, Features: 201239, Train accuracy: 0.990, Dev accuracy: 0.723\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier1 = Classifier(feature1, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier1.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LNlQyjEB4Bwt"
      },
      "outputs": [],
      "source": [
        "def feature2(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "    # BEGIN SOLUTION\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "    negation = {\"not\", \"no\", \"never\", \"n't\"}\n",
        "\n",
        "    for i, w in enumerate(words):\n",
        "      w = w.lower()\n",
        "      if w in negation:\n",
        "        feats[i, w] = 1\n",
        "    return feats\n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JgpuykF67oWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8862db00-211e-4acd-d0e5-75ed4b2876a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature2, Features: 254, Train accuracy: 0.631, Dev accuracy: 0.576\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier2 = Classifier(feature2, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier2.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FmJKucgn4CEg"
      },
      "outputs": [],
      "source": [
        "def feature3(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "    # BEGIN SOLUTION\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for w in words:\n",
        "      w = w.lower()\n",
        "      feats[w] = feats.get(w, 0) + 1\n",
        "\n",
        "    return feats\n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "g_f--utb7q4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c586d71-a313-4488-f795-2d74becdf8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature3, Features: 28057, Train accuracy: 0.863, Dev accuracy: 0.730\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier3 = Classifier(feature3, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier3.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2J1BLgatMP"
      },
      "source": [
        "## Deliverable 2\n",
        "\n",
        "The two cells in \"Combine your features\" will generate a file named `combiner_function_predictions.csv`.\n",
        "\n",
        "> Download this file (using the file manager on the left panel in Colab) and submit this to Gradescope along with your notebook.\n",
        "\n",
        "Please do not change the auto-generated filename!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNW1KboiuFvS"
      },
      "source": [
        "### Combine your features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEpK5LyMgv5c"
      },
      "source": [
        "Next, let's combine any or all the features you have developed into one big model and make predictions on the test data. There is no exact number/threshold we're looking for, accuracy-wise, but the combiner function should *generally* have a higher accuracy than BoW on its own (assuming your features are adding additional information beyond what BoW is adding).\n",
        "\n",
        "You don't need to edit the following cell, unless you want to change which features are handed off to the \"combined\" model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jxKmEqI5JY71"
      },
      "outputs": [],
      "source": [
        "def combiner_function(text):\n",
        "\n",
        "    # Here the `all_feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    # at the moment, all 4 of: bag of words and your 3 original features are handed off to the combined model\n",
        "    # update the values within [bag_of_words, feature1, feature2, feature3] to change this.\n",
        "\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "    all_feats={}\n",
        "    for d in [bag_of_words(text), feature1(text), feature2(text), feature3(text)]:\n",
        "        for k, v in d.items():\n",
        "          all_feats[k] = all_feats.get(k, 0) + v\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    return all_feats\n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "D-tRUFTIdAqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17d09ce-0100-4671-f9f2-5717c726abc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: combiner_function, Features: 224463, Train accuracy: 0.994, Dev accuracy: 0.767\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "big_classifier = Classifier(combiner_function, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "big_classifier.evaluate()\n",
        "\n",
        "#generate csv file with prediction output on test data\n",
        "big_classifier.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfr7Qhs4WWNO"
      },
      "source": [
        "_We recommend that you download `combiner_function_predictions.csv` now in case your runtime gets disconnected._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpdIrv2WuFvT"
      },
      "source": [
        "## Further Exploration: Interrogating classifiers\n",
        "\n",
        "Note: No deliverables are in this section; it's optional. Treat this portion as a useful tool for further understanding the features you worked on and how they affter the classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lgyoJm09pqe"
      },
      "source": [
        "Below you will find several ways in which you can interrogate your model to get ideas on ways to improve its performance.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBWj2zuYuFvX"
      },
      "source": [
        "1. First, let's look at the confusion matrix of its predictions (where we can compare the true labels with the predicted labels). What kinds of mistakes is it making? (While this is mainly helpful in the context of multiclass classification, we can still see if there's a bias toward predicting a specific class in the binary setting as well)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7ulxd1TosIMV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "8e817a88-375c-42a9-cde1-4e9ff89126dd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def print_confusion(classifier):\n",
        "    cm = confusion_matrix(classifier.devY, classifier.log_reg.predict(classifier.devX))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.log_reg.classes_)\n",
        "    disp.plot(xticks_rotation=\"vertical\", values_format=\"d\", cmap='Blues')\n",
        "    plt.show()\n",
        "\n",
        "print_confusion(big_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPhH4flIuEbx"
      },
      "source": [
        "2. Next, let's look at the features that are most important for each of the classes (ranked by how strong their corresponding coefficient is). Do the features you are defining help in the ways you think they should?  Do sets of successful features suggests others, or complementary features that may provide a different view on the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "IAyGuXIi9pqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5fa4d9-b8ce-449b-def0-40556ca57b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos\t2.401\t('not', 'bad')\n",
            "pos\t2.365\t('no', 'wait')\n",
            "pos\t2.213\t('no', 'problem')\n",
            "pos\t2.194\t('not', 'too')\n",
            "pos\t1.806\t(\"n't\", 'bad')\n",
            "pos\t1.699\t(\"n't\", 'too')\n",
            "pos\t1.605\tbomb\n",
            "pos\t1.597\t('never', 'had')\n",
            "pos\t1.579\t('bad', '.')\n",
            "pos\t1.578\t('only', '$')\n",
            "pos\t1.508\t('not', 'only')\n",
            "pos\t1.478\t('anyone', 'else')\n",
            "pos\t1.467\t('never', 'have')\n",
            "pos\t1.429\t('not', 'crowded')\n",
            "pos\t1.429\t('i', 'love')\n",
            "pos\t1.362\t('come', 'to')\n",
            "pos\t1.361\t('no', 'complaints')\n",
            "pos\t1.351\t('no', 'charge')\n",
            "pos\t1.341\t('be', 'disappointed')\n",
            "pos\t1.339\texcited\n",
            "pos\t1.326\t('not', 'expensive')\n",
            "pos\t1.309\t(3, 'never')\n",
            "pos\t1.304\tbeautiful\n",
            "pos\t1.280\t('not', 'here')\n",
            "pos\t1.279\t('to', 'buy')\n",
            "\n",
            "neg\t-2.331\t('not', 'great')\n",
            "neg\t-2.215\t('not', 'good')\n",
            "neg\t-1.893\t('that', 'great')\n",
            "neg\t-1.885\t('and', 'last')\n",
            "neg\t-1.667\tunfortunately\n",
            "neg\t-1.665\t('not', 'worth')\n",
            "neg\t-1.656\t('no', 'thanks')\n",
            "neg\t-1.620\tdisappointing\n",
            "neg\t-1.555\tskeptical\n",
            "neg\t-1.506\t('but', 'here')\n",
            "neg\t-1.476\t('were', 'out')\n",
            "neg\t-1.469\tworst\n",
            "neg\t-1.468\tsadly\n",
            "neg\t-1.461\twarning\n",
            "neg\t-1.461\thated\n",
            "neg\t-1.460\thate\n",
            "neg\t-1.459\t('they', 'need')\n",
            "neg\t-1.459\t('had', 'better')\n",
            "neg\t-1.456\t('so', 'so')\n",
            "neg\t-1.440\thesitant\n",
            "neg\t-1.433\tawful\n",
            "neg\t-1.425\t(\"'ll\", 'try')\n",
            "neg\t-1.406\t('not', 'very')\n",
            "neg\t-1.389\tscary\n",
            "neg\t-1.345\tlacking\n",
            "\n"
          ]
        }
      ],
      "source": [
        "big_classifier.printWeights(n=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80DUsSXu7h9"
      },
      "source": [
        "3. Next, let's look at the individual data points that are the hardest to classify correctly. Does it suggest any features you might create to disentangle them?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "I4uTzwV99pqe"
      },
      "outputs": [],
      "source": [
        "def analyze(classifier):\n",
        "\n",
        "    probs=classifier.log_reg.predict_proba(classifier.devX)\n",
        "    predicts=classifier.log_reg.predict(classifier.devX)\n",
        "\n",
        "    classes={}\n",
        "    for idx, lab in enumerate(classifier.log_reg.classes_):\n",
        "        classes[lab]=idx\n",
        "\n",
        "    mistakes={}\n",
        "    for i in range(len(probs)):\n",
        "        if predicts[i] != classifier.devY[i]:\n",
        "            predicted_lab_idx=classes[predicts[i]]\n",
        "            mistakes[i]=probs[i][predicted_lab_idx]\n",
        "\n",
        "    frame=[]\n",
        "    sorted_x = sorted(mistakes.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k, v in sorted_x:\n",
        "        idd=classifier.devOrig[k][0]\n",
        "        text=classifier.devOrig[k][2]\n",
        "        frame.append([idd, v, classifier.devY[k], predicts[k], text])\n",
        "\n",
        "    df=pd.DataFrame(frame, columns=[\"id\", \"P(predicted class confidence)\", \"Human label\", \"Prediction\", \"Text\"])\n",
        "\n",
        "    with option_context('display.max_colwidth', 400):\n",
        "        display(df.head(n=20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UXmRhSuzxaJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "outputId": "0e190f7c-9df6-48c9-bc15-f0487bfafba8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       id  P(predicted class confidence) Human label Prediction  \\\n",
              "0   41483                       0.999206         neg        pos   \n",
              "1   42167                       0.999126         neg        pos   \n",
              "2   40781                       0.999117         pos        neg   \n",
              "3   41098                       0.999029         pos        neg   \n",
              "4   40853                       0.998704         pos        neg   \n",
              "5   40495                       0.997962         neg        pos   \n",
              "6   41229                       0.997915         pos        neg   \n",
              "7   40109                       0.997029         neg        pos   \n",
              "8   41078                       0.996558         neg        pos   \n",
              "9   42068                       0.993812         neg        pos   \n",
              "10  41242                       0.992420         neg        pos   \n",
              "11  41527                       0.990634         pos        neg   \n",
              "12  41456                       0.990024         neg        pos   \n",
              "13  41858                       0.989581         neg        pos   \n",
              "14  40494                       0.988307         neg        pos   \n",
              "15  41925                       0.986807         neg        pos   \n",
              "16  40018                       0.986409         neg        pos   \n",
              "17  41826                       0.985985         pos        neg   \n",
              "18  41931                       0.984331         neg        pos   \n",
              "19  41054                       0.984238         pos        neg   \n",
              "\n",
              "                                                                                                                                                                                                                                                 Text  \n",
              "0                                                                                                                          He told me to give him a break, but as a customer I told him that it was his duty to fulfill the order I put in perfectly.  \n",
              "1                                                                                                           By the time his replacement steak was brought out sans hair, my plate was clean (minus the rock-tatoes) so he had to eat while I watched.  \n",
              "2                                                                                                                         I went in this Boutique, I couldn't believe I never saw it before..I knew it was gonna be trouble with my shopping problem.  \n",
              "3   I must commend the management....a family of DB's came in with their little dogs....manager \"hey folks we can't have the dogs in here it's a restaurant\"  I imagine there are health codes ect....DB's \" yeah we know, we are eating outside\"????  \n",
              "4                                                                                                                                         They never do more than they need to unless it is requested, so they won't run your wallet into the ground.  \n",
              "5                                                                                                                       But similar to other reviews, they pushed me right into the payment process as soon as I identified a pair of frames I liked.  \n",
              "6                                                                                                                                         Absent someone to tuck you in at night, it is difficult to imagine one would desire anything more in a B&B.  \n",
              "7                                                                                                                      If you live there, you will have to do their job when it comes to maintenance if you want something fixed in a timely fashion.  \n",
              "8                                                                                                                                                                (the twin pullout is cheap and you can feel the springs so have fun sleeping on it).  \n",
              "9                                                                                              She was more than happy to oblige AND NEVER mentioned that it would cost us $98 extra - which is 50% more than the entire charge of the weekly rental!  \n",
              "10                                                                                                                                             I mean Google the thing for F sake and you will see many amazing ways of presenting this awesome dish!  \n",
              "11                                                                                                                                                                           Took a while to get drinks out, but with 18 people that can be expected.  \n",
              "12                                                                                                                                                                          Oh, and you have to love all the hidden rules they like to submit you to.  \n",
              "13                                                              Philly cheese steak was filled with some kind of ground-up meat, mixed with onions and (I think) green bell peppers, soaked in Cheese Whiz on a cold bun, with a cold order of fries.  \n",
              "14                                                                                                                                                       Floaters are managers who do just enough to maintain current numbers and current status quo.  \n",
              "15                                                                                                                                                             ***** Maybe next time have someone with nice handwriting to write on dessert plate!!!!  \n",
              "16                                                                    In fact, we reacted in kindness and tried to counteract the wait staff's inappropriate behaviour with good manners and gentle speech: WE were the ones accommodating the staff.  \n",
              "17                                                                                              Yet it only takes few minutes to get thru the line at Dutch bros because there is someone walking car to car taking orders with a smart phone device.  \n",
              "18                                                                                                                \"Kobe Sliders\" at your local bar and grill for $12 have more in common with an aging basketball player than they do true Kobe Beef.  \n",
              "19                                                                                                                                                                                                        Don't expect to find boring and basic here.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ca56b29-4a85-4360-bce0-aa326ae41da8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P(predicted class confidence)</th>\n",
              "      <th>Human label</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>41483</td>\n",
              "      <td>0.999206</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>He told me to give him a break, but as a customer I told him that it was his duty to fulfill the order I put in perfectly.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>42167</td>\n",
              "      <td>0.999126</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>By the time his replacement steak was brought out sans hair, my plate was clean (minus the rock-tatoes) so he had to eat while I watched.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40781</td>\n",
              "      <td>0.999117</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I went in this Boutique, I couldn't believe I never saw it before..I knew it was gonna be trouble with my shopping problem.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41098</td>\n",
              "      <td>0.999029</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I must commend the management....a family of DB's came in with their little dogs....manager \"hey folks we can't have the dogs in here it's a restaurant\"  I imagine there are health codes ect....DB's \" yeah we know, we are eating outside\"????</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40853</td>\n",
              "      <td>0.998704</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>They never do more than they need to unless it is requested, so they won't run your wallet into the ground.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>40495</td>\n",
              "      <td>0.997962</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>But similar to other reviews, they pushed me right into the payment process as soon as I identified a pair of frames I liked.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>41229</td>\n",
              "      <td>0.997915</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Absent someone to tuck you in at night, it is difficult to imagine one would desire anything more in a B&amp;B.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>40109</td>\n",
              "      <td>0.997029</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>If you live there, you will have to do their job when it comes to maintenance if you want something fixed in a timely fashion.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>41078</td>\n",
              "      <td>0.996558</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>(the twin pullout is cheap and you can feel the springs so have fun sleeping on it).</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>42068</td>\n",
              "      <td>0.993812</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>She was more than happy to oblige AND NEVER mentioned that it would cost us $98 extra - which is 50% more than the entire charge of the weekly rental!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>41242</td>\n",
              "      <td>0.992420</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I mean Google the thing for F sake and you will see many amazing ways of presenting this awesome dish!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>41527</td>\n",
              "      <td>0.990634</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Took a while to get drinks out, but with 18 people that can be expected.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>41456</td>\n",
              "      <td>0.990024</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Oh, and you have to love all the hidden rules they like to submit you to.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>41858</td>\n",
              "      <td>0.989581</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Philly cheese steak was filled with some kind of ground-up meat, mixed with onions and (I think) green bell peppers, soaked in Cheese Whiz on a cold bun, with a cold order of fries.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>40494</td>\n",
              "      <td>0.988307</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Floaters are managers who do just enough to maintain current numbers and current status quo.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>41925</td>\n",
              "      <td>0.986807</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>***** Maybe next time have someone with nice handwriting to write on dessert plate!!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>40018</td>\n",
              "      <td>0.986409</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>In fact, we reacted in kindness and tried to counteract the wait staff's inappropriate behaviour with good manners and gentle speech: WE were the ones accommodating the staff.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>41826</td>\n",
              "      <td>0.985985</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Yet it only takes few minutes to get thru the line at Dutch bros because there is someone walking car to car taking orders with a smart phone device.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>41931</td>\n",
              "      <td>0.984331</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>\"Kobe Sliders\" at your local bar and grill for $12 have more in common with an aging basketball player than they do true Kobe Beef.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>41054</td>\n",
              "      <td>0.984238</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Don't expect to find boring and basic here.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ca56b29-4a85-4360-bce0-aa326ae41da8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8ca56b29-4a85-4360-bce0-aa326ae41da8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8ca56b29-4a85-4360-bce0-aa326ae41da8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"analyze(big_classifier)\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"41483\",\n          \"41826\",\n          \"41925\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"P(predicted class confidence)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005580620298147387,\n        \"min\": 0.9842384703345799,\n        \"max\": 0.9992064182037179,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.9992064182037179,\n          0.9859845754766678,\n          0.9868067472064902\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Human label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"pos\",\n          \"neg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prediction\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neg\",\n          \"pos\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"He told me to give him a break, but as a customer I told him that it was his duty to fulfill the order I put in perfectly.\",\n          \"Yet it only takes few minutes to get thru the line at Dutch bros because there is someone walking car to car taking orders with a smart phone device.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "analyze(big_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwMGcGduh9Af"
      },
      "source": [
        "### Closing and Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations on finishing HW2!"
      ],
      "metadata": {
        "id": "b53y_6-rknCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> For this homework, you will be able to see a leaderboard on Gradescope. This is **optional** (for fun) and to see how your featurized model may compare to your classmates! **You will still be graded on good effort**, not on an autograder criteria.\n",
        "\n"
      ],
      "metadata": {
        "id": "4vUi95EQkNgM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMPmDxDafGTa"
      },
      "source": [
        "There are 3 deliverables, each to be submitted on [Gradescope](https://www.gradescope.com/courses/1238346):\n",
        "\n",
        "1. **HW 2 Test Set Predictions**\n",
        "- Submit the file `combiner_function_predictions.csv`. You can find files associated with the notebook if you click on the folder icon on the left-side panel of your Colab notebook. *Don't alter the file name.*\n",
        "-  `Folder icon` (left panel) --> download `combiner_function_predictions.csv`\n",
        "\n",
        "2. **HW 2 Code**\n",
        "- Download your Colab notebook as an .ipynb file (\n",
        "`File` --> `Download` --> `Download .ipynb`)\n",
        "- Submit `HW2.ipynb`. *Don't alter the file name.*\n",
        "- If you used any external dictionaries, please attach them as well.\n",
        "\n",
        "3. **HW 2 PDF**\n",
        "- If using Colab, you can use the cell below to generate a pdf.\n",
        "\n",
        "Please ensure that you submit the predictions, completed notebook (notebook and pdf), and any external dictionaries used onto [Gradescope](https://www.gradescope.com/courses/1238346) before February 10 at 11:59pm.  The notebook you upload to [Gradescope](https://www.gradescope.com/courses/1238346) must be named `HW2.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXPORT_EXCLUDE#\n",
        "\n",
        "#@markdown This is a helper function to generate a PDF in Colab.\n",
        "#@markdown If you are using Jupyter notebook, you can do `File > Save and Export Notebook as HTML`, then save the resulting HTML file as a PDF.\n",
        "#@markdown Alternatively, in Juypter notebook, you might try `File > Save and Export Notebook as PDF`, but just make sure you already have `pandoc` installed.\n",
        "\n",
        "def colab_export_pdf():\n",
        "    # Modified from: https://medium.com/@jonathanagustin/convert-colab-notebook-to-pdf-0ccd8f847dd6\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "        print(\"This cell only works in Google Colab!\")\n",
        "        print(\"If you are running locally, click File > Export as HTML. Then open the HTML file and save it as a PDF.\")\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(\"Generating PDF. This may take a few seconds.\")\n",
        "        import os, datetime, json, locale, pathlib, urllib, requests, werkzeug, nbformat, google, yaml, warnings\n",
        "        locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "        NAME = pathlib.Path(werkzeug.utils.secure_filename(urllib.parse.unquote(requests.get(f\"http://{os.environ['COLAB_JUPYTER_IP']}:{os.environ['KMP_TARGET_PORT']}/api/sessions\").json()[0][\"name\"])))\n",
        "        TEMP = pathlib.Path(\"/content/pdfs\") / f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{NAME.stem}\"; TEMP.mkdir(parents=True, exist_ok=True)\n",
        "        NB = [cell for cell in nbformat.reads(json.dumps(google.colab._message.blocking_request(\"get_ipynb\", timeout_sec=30)[\"ipynb\"]), as_version=4).cells if \"--Colab2PDF\" not in cell.source]\n",
        "        warnings.filterwarnings('ignore', category=nbformat.validator.MissingIDFieldWarning)\n",
        "        with (TEMP / f\"{NAME.stem}.ipynb\").open(\"w\", encoding=\"utf-8\") as nb_copy: nbformat.write(nbformat.v4.new_notebook(cells=NB or [nbformat.v4.new_code_cell(\"#\")]), nb_copy)\n",
        "        if not pathlib.Path(\"/usr/local/bin/quarto\").exists():\n",
        "            !wget -q \"https://quarto.org/download/latest/quarto-linux-amd64.deb\" -P {TEMP} && dpkg -i {TEMP}/quarto-linux-amd64.deb > /dev/null && quarto install tinytex --update-path --quiet\n",
        "        with (TEMP / \"config.yml\").open(\"w\", encoding=\"utf-8\") as file: yaml.dump({'include-in-header': [{\"text\": r\"\\usepackage{fvextra}\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines,breakanywhere,commandchars=\\\\\\{\\}}\"}],'include-before-body': [{\"text\": r\"\\DefineVerbatimEnvironment{verbatim}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines}\"}]}, file)\n",
        "        !quarto render {TEMP}/{NAME.stem}.ipynb --metadata-file={TEMP}/config.yml --to pdf -M latex-auto-install -M margin-top=1in -M margin-bottom=1in -M margin-left=1in -M margin-right=1in --quiet\n",
        "        google.colab.files.download(str(TEMP / f\"{NAME.stem}.pdf\"))\n",
        "\n",
        "colab_export_pdf()"
      ],
      "metadata": {
        "id": "69gkepFDWD3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bf7de08-346d-4ec1-83f6-78dd6ea0d838"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating PDF. This may take a few seconds.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9216f22e-a944-4d9f-b756-28abc444dab2\", \"HW2.pdf\", 101612)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}